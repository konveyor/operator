---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-proxy
  namespace: "{{ app_namespace }}"
  labels:
    app.kubernetes.io/name: llm-proxy
    app.kubernetes.io/component: kai
    app.kubernetes.io/part-of: tackle
data:
  lightspeed-stack.yaml: |
    # Minimal Lightspeed Stack configuration for running ONLY Llama Stack server
    # This config disables all LCS features and just uses the project to start Llama Stack
    name: Llama Stack Server Only
    service:
      # LCS service won't actually run with this config - we're just using it for Llama Stack
      host: 0.0.0.0
      port: 8080
      auth_enabled: false
      workers: 1
    
    # This is the key - we're NOT using Llama Stack as a library or connecting to external server
    # Instead, we'll use the project's ability to start Llama Stack as a separate server
    llama_stack:
      use_as_library_client: false
      # Dummy URL since we're not actually connecting to anything
      url: http://localhost:8321
      api_key: unused
    
    # Disable all LCS features since we only want Llama Stack
    user_data_collection:
      feedback_enabled: false
      transcripts_enabled: false
    
    # Use in-memory cache (minimal)
    conversation_cache:
      type: "noop"
    
    # No authentication
    authentication:
      module: "noop"
  run.yaml: |
    version: 2
    image_name: starter
    external_providers_dir: /tmp/.llama/providers.d
    apis:
    - inference
    providers:
      inference:
      - provider_id: {{ kai_llm_proxy_provider_id }}
        provider_type: {{ kai_llm_proxy_provider_type }}
        config:
          # All config values use environment variable substitution
          # The deployment injects these from the kai-api-keys secret
{% if kai_llm_proxy_provider_type == 'remote::openai' %}
          api_key: ${env.OPENAI_API_KEY}
{% if kai_llm_baseurl %}
          base_url: {{ kai_llm_baseurl }}
{% endif %}
{% elif kai_llm_proxy_provider_type == 'remote::azure-openai' %}
          api_key: ${env.AZURE_OPENAI_API_KEY}
{% if kai_llm_baseurl %}
          base_url: {{ kai_llm_baseurl }}
{% else %}
          base_url: ${env.AZURE_OPENAI_ENDPOINT}
{% endif %}
          api_version: ${env.AZURE_OPENAI_API_VERSION:2024-02-15-preview}
{% elif kai_llm_proxy_provider_type == 'remote::google' %}
          api_key: ${env.GOOGLE_API_KEY}
{% if kai_llm_baseurl %}
          base_url: {{ kai_llm_baseurl }}
{% endif %}
{% elif kai_llm_proxy_provider_type == 'remote::anthropic' %}
          api_key: ${env.ANTHROPIC_API_KEY}
{% if kai_llm_baseurl %}
          base_url: {{ kai_llm_baseurl }}
{% endif %}
{% elif kai_llm_proxy_provider_type == 'remote::bedrock' %}
          aws_access_key_id: ${env.AWS_ACCESS_KEY_ID}
          aws_secret_access_key: ${env.AWS_SECRET_ACCESS_KEY}
          aws_session_token: ${env.AWS_SESSION_TOKEN}
          region: ${env.AWS_DEFAULT_REGION:us-east-1}
{% else %}
          # Generic provider - expects API_KEY and optionally BASE_URL in the secret
          api_key: ${env.API_KEY}
{% if kai_llm_baseurl %}
          base_url: {{ kai_llm_baseurl }}
{% endif %}
{% endif %}
    storage:
      backends:
        kv_default:
          type: kv_postgres
          user: kai
          password: ${env.POSTGRESQL_PASSWORD}
          host: {{ kai_database_address }}
          port: 5432
          db: kai
          table_prefix: llm_proxy_kv_
        sql_default:
          type: sql_postgres
          user: kai
          password: ${env.POSTGRESQL_PASSWORD}
          host: {{ kai_database_address }}
          port: 5432
          db: kai
          table_prefix: llm_proxy_
      stores:
        metadata:
          namespace: registry
          backend: kv_default
        inference:
          table_name: llm_proxy_inference_store
          backend: sql_default
          max_write_queue_size: 10000
          num_writers: 4
        conversations:
          table_name: llm_proxy_conversations
          backend: sql_default
    registered_resources:
      models:
{% if kai_llm_model %}
      - model_id: proxied-model
        provider_id: {{ kai_llm_proxy_provider_id }}
        model_type: llm
        provider_model_id: {{ kai_llm_model }}
{% else %}
      []
{% endif %}
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups: []
    server:
      port: 8321
{% if feature_auth_required %}
      auth:
        provider_config:
          type: "oauth2_token"
          # Skip TLS verification for self-signed certificates (same as hub does)
          verify_tls: false
          jwks:
            # Use the same protocol and base URL that the hub uses for Keycloak
            uri: "{{ keycloak_sso_url }}/auth/realms/{{ keycloak_sso_realm }}/protocol/openid-connect/certs"
          # The issuer must match exactly what's in the JWT token from hub auth
          issuer: "{{ keycloak_sso_url }}/auth/realms/{{ keycloak_sso_realm }}"
          audience: "{{ keycloak_api_audience }}"
{% endif %}
    telemetry:
      enabled: false